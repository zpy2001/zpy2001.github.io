---
title: 论文笔记 - aceso：通过迭代缓解瓶颈实现高效的并行DNN训练
description: Article Note - Aceso Efficient Parallel DNN Training through Iterative Bottleneck Alleviation
slug: research-mlsys-Aceso
date: 2024-01-21 17:04:00+0000
categories:
    - MLsys
tags:
    - research
    - MLsys
    - CS
    - ArticleNotes
weight: 1
---

# 论文笔记 - aceso：通过迭代缓解瓶颈实现高效的并行DNN训练

DNN并行加速

- **数据并行**：数据并行在设备之间复制运算符和参数张量，以并行处理分割的mini-batch训练数据，但需要在每次迭代结束时进行参数同步通信。
- **张量并行**：张量并行[18,43,48]将权重张量分割到设备之间，以支持大型参数模型，但需要激活张量通信
- **流水线并行**：流水线并行[10,15]将模型运算符分组为连续阶段，每个阶段分配给一个设备，并通过简单的发送-接收通信连接。为了提高硬件利用率，每个mini-batch进一步划分为microbatch。它采用精心设计的调度算法来协调不同microbatch之间的运算符执行
- **重新计算**：为了节省内存，重新计算机制[5,20]在运算符的计算输出后释放输出张量，并在稍后的梯度计算中“恢复”它，通过额外的运算符计算进行梯度计算
- **机制组合**：为了发挥不同机制的优势，当前的解决方案[43,44,49]层次性地组合了数据/张量/管道并行和重新计算进行分布式执行

总的可能配置数量随着运算符和机制的数量呈指数增长，寻找最佳并行化配置是NP-hard问题

现有的并行系统要么搜索一个有限的配置子空间，专注于少数几个机制（其余手动配置），要么以较粗的粒度探索模型

寻找最优配置：

- 基于规则的解决方案：为特定模型预定义了一种策略
- 自动搜索解决方案：使用数学规划（如动态规划和整数线性规划）自动配置并行机制，搜索空间太大